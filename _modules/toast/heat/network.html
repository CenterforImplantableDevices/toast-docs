<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>toast.heat.network &mdash; toast 1.3.2 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../toast.html" class="icon icon-home"> toast
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../toast.bread.html">toast.bread</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../toast.butter.html">toast.butter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../toast.heat.html">toast.heat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../toast.kitchen.html">toast.kitchen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../toast.misc.html">toast.misc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../toast.plate.html">toast.plate</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../toast.html">toast</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../toast.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>toast.heat.network</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for toast.heat.network</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">t_optim</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">percentile</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="nb">max</span> <span class="k">as</span> <span class="n">np_max</span><span class="p">,</span> <span class="nb">min</span> <span class="k">as</span> <span class="n">np_min</span>
<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">join</span><span class="p">,</span> <span class="n">exists</span>
<span class="kn">from</span> <span class="nn">toast.misc</span> <span class="kn">import</span> <span class="n">ttime</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">remove</span><span class="p">,</span> <span class="n">mkdir</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">sys</span> <span class="kn">import</span> <span class="n">maxsize</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">time</span>



<div class="viewcode-block" id="Network"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network">[docs]</a><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        A Class that contains all the functionality and tracking necessary to train and debug Deep Neural Networks.</span>
<span class="sd">    &#39;&#39;&#39;</span>

<div class="viewcode-block" id="Network.__init__"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_type_dict</span><span class="o">=</span><span class="p">{},</span> <span class="n">path_save_progress</span><span class="o">=</span><span class="s1">&#39;./&#39;</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">description_short</span><span class="o">=</span><span class="s1">&#39;toast&#39;</span><span class="p">,</span> <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Initialization function to create an instance of data.</span>

<span class="sd">        Args:  </span>
<span class="sd">            layer_type_dict (dict, optional):   Utility attribute to declare the expected layer types of the network. Helps to organize debugging/tracking functoinality</span>
<span class="sd">            path_save_progress (str, optional): Path to the parent directory where progress and tracking data should be saved.</span>
<span class="sd">            description (str, optional):        Description describing the contents/structure of the network for easy reference.</span>
<span class="sd">            description_short (str, optional):  Short few-character description describing the network type, to be inserted into filenames.</span>
<span class="sd">            comment (str, optional):            Comment about what this network is being used to do/test/investigate for later identification of tests run.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An instance of the Class class representing a deep neural network.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Officially define the layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_type_dict</span> <span class="o">=</span> <span class="n">layer_type_dict</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">registered_forward_hooks</span>   <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">registered_backward_hooks</span>  <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">track_activation_setup</span>     <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">track_activationGrad_setup</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>

        <span class="c1"># Debugging and tracking attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_best</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span> <span class="n">maxsize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span>                 <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_hidden</span>        <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">falsepos_hidden</span>        <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">falseneg_hidden</span>        <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_train_history</span>     <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_test_history</span>      <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_history</span>  <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falsepos_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falseneg_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falsepos_history</span>  <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falseneg_history</span>  <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">new_layers_added</span><span class="p">()</span>

        <span class="c1"># Variables important for network identification and saving</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">creation_ts</span>       <span class="o">=</span> <span class="n">ttime</span><span class="o">.</span><span class="n">get_timestamp</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">description</span>       <span class="o">=</span> <span class="n">description</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">description_short</span> <span class="o">=</span> <span class="n">description_short</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">comment</span>           <span class="o">=</span> <span class="n">comment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_path_prefix</span>  <span class="o">=</span> <span class="n">path_save_progress</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_save_paths</span><span class="p">(</span><span class="n">create_folders</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_regenerator</span> <span class="o">=</span> <span class="kc">False</span></div>


<div class="viewcode-block" id="Network.forward"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;This method intended to be overwritten by child class.  Default activity is defined below</span>
<span class="sd">        Overriding the nn.Module forward function to process data based on our network structure.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Data to be processed by the network.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The output after passing ``data`` through the network.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">WARNING: REMEMBER TO OVERWRITE THE forward FUNCTION</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">0</span></div>


<div class="viewcode-block" id="Network.convert_to_decision"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.convert_to_decision">[docs]</a>    <span class="k">def</span> <span class="nf">convert_to_decision</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;This method intended to be overwritten by child class.  Default activity is defined below</span>
<span class="sd">        Convertrs a network output to a definite decision based on thresholds highest likelihood values.</span>

<span class="sd">        Args:</span>
<span class="sd">            output: Value output by network</span>

<span class="sd">        Returns:</span>
<span class="sd">            Float Tensor representing the final decision of the network.</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">WARNING: REMEMBER TO OVERWRITE THE convert_to_decision FUNCTION</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">output</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span></div>


<div class="viewcode-block" id="Network.new_layers_added"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.new_layers_added">[docs]</a>    <span class="k">def</span> <span class="nf">new_layers_added</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Resets all the layer-specific tracking attributes of the network object to match new layer dimensions. Can also be used to reset tracking variables if needed.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_avg</span>          <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_std</span>          <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q1</span>           <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q3</span>           <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_min</span>          <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_max</span>          <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_avg_history</span>  <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_std_history</span>  <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q1_history</span>   <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q3_history</span>   <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_min_history</span>  <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_max_history</span>  <span class="o">=</span> <span class="p">{</span> <span class="n">p</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_avg</span>         <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_std</span>         <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q1</span>          <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q3</span>          <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_min</span>         <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_max</span>         <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_avg_history</span> <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_std_history</span> <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q1_history</span>  <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q3_history</span>  <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_min_history</span> <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_max_history</span> <span class="o">=</span> <span class="p">{</span> <span class="n">l</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_of_layers</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradCAM_latest</span>           <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradCAM_expanded_latest</span>  <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradCAM_alpha_latest</span>     <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Network.train_with"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.train_with">[docs]</a>    <span class="k">def</span> <span class="nf">train_with</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_to_train</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span> <span class="n">save_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">track_accuracy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keep_history</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_timings</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Performs one epoch of training, while also tracking/logging data that has been set to be tracked/logged.</span>

<span class="sd">        Args:  </span>
<span class="sd">            dataloader_to_train:                   Dataloader that inherits from torch.utils.data.Dataset to be used during training. Typically a Dataset_Bread instance from a Data_Bread.training class attribute</span>
<span class="sd">            learning_rate (float, optional):       Learning rate to use during training. Typical value is 1e-3</span>
<span class="sd">            momentum (float, optional):            Momentum coefficient.  Typical value is on the order of 0.8</span>
<span class="sd">            weight_decay (float, optional):        Weight decay coefficient.  Typical value is on the order of 1e-2</span>
<span class="sd">            loss_fn (torch.nn loss fcn, optional): Torch.nn derived loss function to use when calculating loss</span>
<span class="sd">            optimizer (str, optional):             Optimizer function to use. Current supported optimizers are: sgd, rmsprop, adam, adagrad, asgd, rprop, adadelta, adamw, adamax.</span>
<span class="sd">            save_progress (bool, optional):        Whether to save the network after the epoch. Will save the network and also replace the &quot;best&quot; network file if performance is better. </span>
<span class="sd">            track_accuracy (bool, optional):       Whether to track the accuracy during training. Typically false.</span>
<span class="sd">            keep_history (bool, optional):          Whether to append tracked variables to the network history</span>
<span class="sd">            show_timings (bool, optional):         Whether to show the timings of each step performed during training. Used for debug purposes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No return value. Updates the parameters/weights of the network and append any tracked variables, as requested.</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="c1"># Define the optimizer function</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span> <span class="n">optimizer</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>      <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;rmsprop&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span>  <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>     <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;adagrad&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">(</span>  <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;asgd&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">ASGD</span><span class="p">(</span>     <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;rprop&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">Rprop</span><span class="p">(</span>    <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;adadelta&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">Adadelta</span><span class="p">(</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;adamw&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>    <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;adamax&#39;</span><span class="p">:</span>
                <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">Adamax</span><span class="p">(</span>   <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span> <span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">),</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
            <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">WARNING: Optimizer &#39;</span> <span class="o">+</span> <span class="n">optimizer</span> <span class="o">+</span> <span class="s1">&#39; not recognized.  Using defauld SGD optimizer&#39;</span><span class="p">)</span>
            <span class="n">optimizer_fn</span> <span class="o">=</span> <span class="n">t_optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>

        <span class="c1"># Define/reset Tracking Variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_train</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">track_accuracy</span><span class="p">:</span>
            <span class="n">number_samples</span>      <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falsepos</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falseneg</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Set network to train mode, which will turn on dropout layers, track the gradients/computation graphs, etc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="c1"># Train for one full iteration of the samples provided</span>
        <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">samples</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloader_to_train</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t\t</span><span class="s1">Time to get data:</span><span class="se">\t</span><span class="si">{:&gt;9.3f}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
                <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="n">samples</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t\t</span><span class="s1">Time to move to GPU:</span><span class="se">\t</span><span class="si">{:&gt;9.3f}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
                <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="n">optimizer_fn</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span> <span class="n">samples</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t\t</span><span class="s1">Time to run forward:</span><span class="se">\t</span><span class="si">{:&gt;9.3f}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
                <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_regenerator</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

            <span class="c1"># Track variables that are requested</span>
            <span class="k">if</span> <span class="n">track_accuracy</span><span class="p">:</span>
                <span class="n">number_samples</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
                <span class="n">label_compare</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_to_decision</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_to_decision</span><span class="p">(</span> <span class="n">labels</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train</span>          <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label_compare</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falsepos</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label_compare</span>  <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falseneg</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label_compare</span>  <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
                <span class="n">number_layers</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()])</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------&#39;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">num</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">num</span> <span class="o">==</span> <span class="n">number_layers</span><span class="o">-</span><span class="mi">2</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="s1">&#39;: &#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
                        <span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sampels: &#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="n">samples</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;labels:  &#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="n">labels</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Outputs: &#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="n">outputs</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loss:    &#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)))</span>
                <span class="c1"># input(&#39;Press [Enter] to continue...&#39;)</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer_fn</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t\t</span><span class="s1">Time to run backward:</span><span class="se">\t</span><span class="si">{:&gt;9.3f}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
                <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">loss_train</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Finish calculating tracked variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_train</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader_to_train</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">keep_history</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_train_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_train</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">track_accuracy</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train</span>          <span class="o">/=</span> <span class="n">number_samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falsepos</span> <span class="o">/=</span> <span class="n">number_samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falseneg</span> <span class="o">/=</span> <span class="n">number_samples</span>
            <span class="k">if</span> <span class="n">keep_history</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>          <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falsepos_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falsepos</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falseneg_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accuracy_train_falseneg</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">save_progress</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_save_paths</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">path_chpt</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_train</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_best</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_best</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_train</span>
            <span class="k">if</span> <span class="n">save_progress</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_save_paths</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">path_best</span><span class="p">)</span></div>
    

<div class="viewcode-block" id="Network.test_with"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.test_with">[docs]</a>    <span class="k">def</span> <span class="nf">test_with</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_to_test</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span> <span class="n">track_accuracy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">track_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">track_activations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">track_activationsGrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keep_history</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_timings</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Tests network performance through one iteration of the given dataset, while also tracking/logging data that has been set to be tracked/logged.</span>

<span class="sd">        Args:  </span>
<span class="sd">            dataloader_to_test:                     Dataloader that inherits from torch.utils.data.Dataset to be used during testing. Typically a Dataset_Bread instance from a Data_Bread.testing class attribute</span>
<span class="sd">            loss_fn (torch.nn loss fcn, optional):  Torch.nn derived loss function to use when calculating loss</span>
<span class="sd">            track_accuracy (bool, optional):        Whether to track the accuracy during training. Typically false.</span>
<span class="sd">            track_weights (bool, optional):         Whether to log the the weights during training. Typically false.</span>
<span class="sd">            track_activations (bool, optional):     Whether to track activation values during forward propogation of network. Typically false.</span>
<span class="sd">            track_activationsGrad (bool, optional): Whether to track activation gradient values from backpropogation. Necessary for some visualizatoin features, like GradCAM. Typically false.</span>
<span class="sd">            keep_history (bool, optional):          Whether to append tracked variables to the network history</span>
<span class="sd">            show_timings (bool, optional):          Whether to show the timings of each step performed during training. Used for debug purposes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No return value.  Will append any tracked variables, as requested.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># Define/reset Tracking Variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_test</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">track_accuracy</span><span class="p">:</span>
            <span class="n">number_samples_tested</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test</span>    <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falsepos</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falseneg</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">track_activations</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_activation_setup</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">WARNING: TRACK ACTIVATIONS NOT SETUP. SKIPPING TRACKING DURING TEST&#39;</span><span class="p">)</span>
                <span class="n">track_activations</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations_keys_ordered</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">track_activationsGrad</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_activationGrad_setup</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">WARNING: TRACK ACTIVATIONS NOT SETUP. SKIPPING TRACKING DURING TEST&#39;</span><span class="p">)</span>
                <span class="n">track_activationsGrad</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_raw</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_keys_ordered</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Set network to eval mode, which will turn off dropout layers, stop tracking the gradients/computation graphs, etc for faster performance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># Test one full iteration of the samples provided</span>
            <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">samples</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloader_to_test</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t\t</span><span class="s1">Time to get data:</span><span class="se">\t</span><span class="si">{:&gt;9.3f}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
                    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

                <span class="n">samples</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t\t</span><span class="s1">Time to move to GPU:</span><span class="se">\t</span><span class="si">{:&gt;9.3f}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
                    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span> <span class="n">samples</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t\t</span><span class="s1">Time to run forward:</span><span class="se">\t</span><span class="si">{:&gt;9.3f}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
                    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

                <span class="c1"># Track the variables that are requested</span>
                <span class="k">if</span> <span class="n">track_accuracy</span><span class="p">:</span>
                    <span class="n">number_samples_tested</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
                    <span class="n">label_compare</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_to_decision</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_to_decision</span><span class="p">(</span> <span class="n">labels</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test</span>          <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label_compare</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falsepos</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label_compare</span>  <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falseneg</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label_compare</span>  <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_regenerator</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">loss_test</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">show_timings</span><span class="p">:</span>
                    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                
        <span class="c1"># Finish calculating tracked variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_test</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader_to_test</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">keep_history</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_test_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_test</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">track_accuracy</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test</span>          <span class="o">/=</span> <span class="n">number_samples_tested</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falsepos</span> <span class="o">/=</span> <span class="n">number_samples_tested</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falseneg</span> <span class="o">/=</span> <span class="n">number_samples_tested</span>
            <span class="k">if</span> <span class="n">keep_history</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>          <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falsepos_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falsepos</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falseneg_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accuracy_test_falseneg</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">track_weights</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">layer_param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">raw_weight_value</span> <span class="o">=</span> <span class="n">layer_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="c1"># raw_weight_value = deepcopy( layer_weights.data).cpu().numpy() # If GPU memory becomes an issue, use this line</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_avg</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">raw_weight_value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_std</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="p">(</span> <span class="n">raw_weight_value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q1</span><span class="p">[</span> <span class="n">layer_param</span><span class="p">]</span> <span class="o">=</span> <span class="n">percentile</span><span class="p">(</span> <span class="n">raw_weight_value</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q3</span><span class="p">[</span> <span class="n">layer_param</span><span class="p">]</span> <span class="o">=</span> <span class="n">percentile</span><span class="p">(</span> <span class="n">raw_weight_value</span><span class="p">,</span> <span class="mi">75</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_min</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]</span> <span class="o">=</span> <span class="n">np_min</span><span class="p">(</span> <span class="n">raw_weight_value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_max</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]</span> <span class="o">=</span> <span class="n">np_max</span><span class="p">(</span> <span class="n">raw_weight_value</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">keep_history</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_avg_history</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_test_avg</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_std_history</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_test_std</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q1_history</span><span class="p">[</span> <span class="n">layer_param</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q1</span><span class="p">[</span> <span class="n">layer_param</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q3_history</span><span class="p">[</span> <span class="n">layer_param</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_test_q3</span><span class="p">[</span> <span class="n">layer_param</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_min_history</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_test_min</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_test_max_history</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_test_max</span><span class="p">[</span><span class="n">layer_param</span><span class="p">]))</span>
        
        <span class="k">if</span> <span class="n">track_activations</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">layer_key</span><span class="p">,</span> <span class="n">layer_activations</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">raw_activations_value</span> <span class="o">=</span> <span class="n">layer_activations</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_avg</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">raw_activations_value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_std</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="p">(</span> <span class="n">raw_activations_value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q1</span><span class="p">[</span> <span class="n">layer_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">percentile</span><span class="p">(</span> <span class="n">raw_activations_value</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q3</span><span class="p">[</span> <span class="n">layer_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">percentile</span><span class="p">(</span> <span class="n">raw_activations_value</span><span class="p">,</span> <span class="mi">75</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_min</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np_min</span><span class="p">(</span> <span class="n">raw_activations_value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_max</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np_max</span><span class="p">(</span> <span class="n">raw_activations_value</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">keep_history</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_avg_history</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activations_test_avg</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_std_history</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activations_test_std</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q1_history</span><span class="p">[</span> <span class="n">layer_key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q1</span><span class="p">[</span> <span class="n">layer_key</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q3_history</span><span class="p">[</span> <span class="n">layer_key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activations_test_q3</span><span class="p">[</span> <span class="n">layer_key</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_min_history</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activations_test_min</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">activations_test_max_history</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activations_test_max</span><span class="p">[</span><span class="n">layer_key</span><span class="p">]))</span></div>

        
<div class="viewcode-block" id="Network.get_labeled_outputs_from"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.get_labeled_outputs_from">[docs]</a>    <span class="k">def</span> <span class="nf">get_labeled_outputs_from</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_to_test</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Will return true labels and corresponding output values from a dataloader for reference</span>

<span class="sd">        Args:</span>
<span class="sd">            dataloader_to_test: Dataloader that inherits from torch.utils.data.Dataset to be used to find labels &amp; outputs. Typically a Dataset_Bread instance from a Data_Bread.testing class attribute</span>

<span class="sd">        Returns:</span>
<span class="sd">            labels (numpy array), outputs (numpy array):  **labels**: Numpy array of labels used during training.  Note if one_hot encoding was used, one_hot labels will be returned.</span>
<span class="sd">            **outputs**: Numpy array of the values output from the network</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">all_labels</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>
        <span class="n">all_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">()</span>

        <span class="c1"># Set network to eval mode, which will turn off dropout layers, stp[ tracking the gradients/computation graphs, etc for faster performance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">samples</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloader_to_test</span><span class="p">:</span>
                <span class="n">samples</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span> <span class="n">samples</span><span class="p">)</span>

                <span class="n">all_labels</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span> <span class="n">all_labels</span><span class="p">,</span>  <span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>
                <span class="n">all_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>
                
        <span class="k">return</span> <span class="n">all_labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">all_outputs</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span></div>


<div class="viewcode-block" id="Network.save_checkpoint"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.save_checkpoint">[docs]</a>    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Saves a &quot;checkpoint&quot; of the network parameters/state so that it can be loaded in case of failure during training.</span>
<span class="sd">        Note that history/tracked variables are not saved or loaded. This is a reduced file-size saving method that only saves what is necessary to checkpoint performance.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str, optional): Path (including filename) to save the network to</span>

<span class="sd">        Returns:</span>
<span class="sd">            No return value. Network will be saved to a new file (or overwrite existing file of same name)</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_save_paths</span><span class="p">()</span>
            <span class="n">path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">path_network</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span> <span class="n">path</span> <span class="o">+</span> <span class="s1">&#39;.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span></div>


<div class="viewcode-block" id="Network.load_checkpoint"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.load_checkpoint">[docs]</a>    <span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Loads a network from a saved &quot;checkpoint&quot;, if saved using above function.</span>
<span class="sd">        Note that history/tracked variables are not saved or loaded. &quot;checkpoint&quot;s are a reduced file-size saving method that only saves what is necessary to checkpoint performance.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str, optional): path (including filename) to a pickle file containing a saved network.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Returns 1 if successful. Saved parameters/attributes will be loaded into the network.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span> <span class="n">path</span><span class="o">+</span><span class="s1">&#39;.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span> <span class="n">state_dict</span><span class="p">)</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ERROR loading model from &#39;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
            <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span></div>
    

<div class="viewcode-block" id="Network.save_network"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.save_network">[docs]</a>    <span class="k">def</span> <span class="nf">save_network</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">remove_checkpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Saves the entire network instance for later reference.  All values, parameters, and attributes are pickled saved.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str, optional):               Path (including filename) to save the network to</span>
<span class="sd">            remove_checkpoint (bool, optional): Whehter to remove existing checkpoints</span>

<span class="sd">        Returns:</span>
<span class="sd">            No return value. Network will be saved to a new file (or overwrite existing file of same name)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remove_all_hooks</span><span class="p">()</span> <span class="c1"># Some hooks are unpickleable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_save_paths</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">path_network</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span> <span class="n">path</span><span class="o">+</span><span class="s1">&#39;.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">remove_checkpoint</span><span class="p">:</span>
            <span class="n">remove</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">path_chpt</span><span class="o">+</span><span class="s1">&#39;.pkl&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">1</span></div>
    

<div class="viewcode-block" id="Network.update_save_paths"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.update_save_paths">[docs]</a>    <span class="k">def</span> <span class="nf">update_save_paths</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">create_folders</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Updates the network attributes representing save paths. Used when attributes are edited after initialization that are used in the save_path definitions.</span>

<span class="sd">        Args:</span>
<span class="sd">            create_folders (bool, optional): Whether to create any mising folders for saved networks.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No return value. All path attributes will be updated.</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="n">dirname_checkpoints</span> <span class="o">=</span> <span class="s1">&#39;network_train_checkpoint&#39;</span>
        <span class="k">if</span> <span class="n">create_folders</span> <span class="ow">and</span> <span class="ow">not</span><span class="p">(</span> <span class="n">exists</span><span class="p">(</span> <span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_path_prefix</span><span class="p">,</span> <span class="n">dirname_checkpoints</span><span class="p">))):</span>
            <span class="n">mkdir</span><span class="p">(</span> <span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_path_prefix</span><span class="p">,</span> <span class="n">dirname_checkpoints</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">path_chpt</span> <span class="o">=</span> <span class="n">join</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_path_prefix</span><span class="p">,</span> <span class="n">dirname_checkpoints</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">description_short</span> <span class="o">+</span> <span class="s1">&#39;_checkpoint_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">description</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">creation_ts</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">comment</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">path_best</span> <span class="o">=</span> <span class="n">join</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_path_prefix</span><span class="p">,</span> <span class="n">dirname_checkpoints</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">description_short</span> <span class="o">+</span> <span class="s1">&#39;_best_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">description</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">creation_ts</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">comment</span><span class="p">)</span>
        <span class="n">dirname_trained</span> <span class="o">=</span> <span class="s1">&#39;network_trained&#39;</span>
        <span class="k">if</span> <span class="n">create_folders</span> <span class="ow">and</span> <span class="ow">not</span><span class="p">(</span> <span class="n">exists</span><span class="p">(</span> <span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_path_prefix</span><span class="p">,</span> <span class="n">dirname_trained</span><span class="p">))):</span>
            <span class="n">mkdir</span><span class="p">(</span> <span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_path_prefix</span><span class="p">,</span> <span class="n">dirname_trained</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">path_network</span> <span class="o">=</span> <span class="n">join</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_path_prefix</span><span class="p">,</span> <span class="n">dirname_trained</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">description_short</span> <span class="o">+</span> <span class="s1">&#39;_network_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">description</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">creation_ts</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">comment</span><span class="p">)</span></div>


<div class="viewcode-block" id="Network.get_list_of_layers"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.get_list_of_layers">[docs]</a>    <span class="k">def</span> <span class="nf">get_list_of_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Returns an ordered list of the layer objects used by the network. These describe the network topology/organization used by the network.</span>

<span class="sd">        Args:</span>
<span class="sd">            No Arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An ordered list of the layer objects found. Note ``nn.Sequential`` objects are ignored in this search.</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="n">identified_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Define a function that will be used recursively</span>
        <span class="k">def</span> <span class="nf">find_and_add_layers</span><span class="p">(</span> <span class="n">network</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            FIXME: Use this function instead:</span>
<span class="sd">                        def modules(self) -&gt; Iterator[&#39;Module&#39;]:</span>
<span class="sd">                            r&quot;&quot;&quot;Returns an iterator over all modules in the network.</span>
<span class="sd">                            Yields:</span>
<span class="sd">                                Module: a module in the network</span>
<span class="sd">                            Note:</span>
<span class="sd">                                Duplicate modules are returned only once. In the following</span>
<span class="sd">                                example, ``l`` will be returned only once.</span>
<span class="sd">                            Example::</span>
<span class="sd">                                &gt;&gt;&gt; l = nn.Linear(2, 2)</span>
<span class="sd">                                &gt;&gt;&gt; net = nn.Sequential(l, l)</span>
<span class="sd">                                &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>
<span class="sd">                                        print(idx, &#39;-&gt;&#39;, m)</span>
<span class="sd">                                0 -&gt; Sequential(</span>
<span class="sd">                                (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">                                (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">                                )</span>
<span class="sd">                                1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">                            &quot;&quot;&quot;</span>
<span class="sd">                            for name, module in self.named_modules():</span>
<span class="sd">                                yield module</span>

<span class="sd">            &#39;&#39;&#39;</span>
            <span class="c1"># Iterate through all the attached modules add them to the identified_layers list</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">network</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># Do not add sequential layers, but iterate within them</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span> <span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
                    <span class="n">find_and_add_layers</span><span class="p">(</span> <span class="n">layer</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">identified_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">layer</span><span class="p">)</span>
        
        <span class="c1"># Call the recursive function on self network object</span>
        <span class="n">find_and_add_layers</span><span class="p">(</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">identified_layers</span></div>


<div class="viewcode-block" id="Network.get_param_modules"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.get_param_modules">[docs]</a>    <span class="k">def</span> <span class="nf">get_param_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Returns an ordered list of the parameter modules contained in the network. These describe the weight values contained by the network.</span>
<span class="sd">            </span>
<span class="sd">        Note:</span>
<span class="sd">            Linear layers contain a weights parameter module and a bias parameter module. (1 layer, 2 parameter modules)</span>
<span class="sd">            </span>
<span class="sd">            Non-linearity layers do not contain parameters as they only perform a fixed operation.</span>

<span class="sd">        Args:</span>
<span class="sd">            No Arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An ordered list of the parameter modules found.</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="n">identified_modules</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Define a function that will be used recursively</span>
        <span class="k">def</span> <span class="nf">find_and_add_modules</span><span class="p">(</span> <span class="n">input_module</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39;Returns an ordered list of the module parameter objects used by the network. These describe the parameters used by the network.</span>

<span class="sd">            Args:</span>
<span class="sd">                input_module (module): Network or smaller module object</span>

<span class="sd">            Returns:</span>
<span class="sd">                An ordered list of the module parameters</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">input_module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># Recurse through a module object until it is None so that we know nothing else is contained or embedded in submodules</span>
                <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">find_and_add_modules</span><span class="p">(</span> <span class="n">module</span><span class="p">)</span>
                <span class="sd">&#39;&#39;&#39;Internal Function used to recursively append modules to identified_modules variable</span>
<span class="sd">                FIXME: Use this function instead: </span>

<span class="sd">                            de.f parameters(self, recurse: bool = True) -&gt; Iterator[Parameter]:</span>
<span class="sd">                                r&quot;&quot;&quot;Returns an iterator over module parameters.</span>
<span class="sd">                                This is typically passed to an optimizer.</span>
<span class="sd">                                Args:</span>
<span class="sd">                                    recurse (bool): if True, then yields parameters of this module</span>
<span class="sd">                                        and all submodules. Otherwise, yields only parameters that</span>
<span class="sd">                                        are direct members of this module.</span>
<span class="sd">                                Yields:</span>
<span class="sd">                                    Parameter: module parameter</span>
<span class="sd">                                Example::</span>
<span class="sd">                                    &gt;&gt;&gt; for param in model.parameters():</span>
<span class="sd">                                    &gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="sd">                                    &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="sd">                                    &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
<span class="sd">                                &quot;&quot;&quot;</span>
<span class="sd">                                for name, param in self.named_parameters(recurse=recurse):</span>
<span class="sd">                                    yield param</span>
<span class="sd">                &#39;&#39;&#39;</span>
                <span class="c1"># Once a module has no submodules, add all its parameters</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">identified_modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">module</span><span class="p">)</span>
        
        <span class="c1"># Recurse through a module object until it is None so that we know nothing else is contained or embedded in submodules</span>
        <span class="n">find_and_add_modules</span><span class="p">(</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">identified_modules</span></div>


<div class="viewcode-block" id="Network.remove_forward_hooks"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.remove_forward_hooks">[docs]</a>    <span class="k">def</span> <span class="nf">remove_forward_hooks</span><span class="p">(</span> <span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Unregisters all forward hooks.</span>

<span class="sd">        May be necessary to remove hooks to move network to/from GPU or to save network, as some implemented hooks are unpickleable.</span>

<span class="sd">        Args:</span>
<span class="sd">            No Arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No Return Value. Hooks will be unregistered.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">track_activation_setup</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">registered_forward_hooks</span><span class="p">:</span>
            <span class="n">hook</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span></div>


<div class="viewcode-block" id="Network.remove_backward_hooks"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.remove_backward_hooks">[docs]</a>    <span class="k">def</span> <span class="nf">remove_backward_hooks</span><span class="p">(</span> <span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Unregisters all backwards hooks. </span>
<span class="sd">        </span>
<span class="sd">        May be necessary to remove hooks to move network to/from GPU or to save network, as some implemented hooks are unpickleable.</span>

<span class="sd">        Args:</span>
<span class="sd">            No Arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No Return Value. Hooks will be unregistered.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">registered_backward_hooks</span><span class="p">:</span>
            <span class="n">hook</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span></div>


<div class="viewcode-block" id="Network.remove_all_hooks"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.remove_all_hooks">[docs]</a>    <span class="k">def</span> <span class="nf">remove_all_hooks</span><span class="p">(</span> <span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Unregisters all forward and backwards hooks. </span>

<span class="sd">        May be necessary to remove hooks to move network to/from GPU or to save network, as some implemented hooks are unpickleable.</span>

<span class="sd">        Args:</span>
<span class="sd">            No Arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No Return Value. Hooks will be unregistered.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remove_forward_hooks</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remove_backward_hooks</span><span class="p">()</span></div>


<div class="viewcode-block" id="Network._add_forwardHook_to_layers"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network._add_forwardHook_to_layers">[docs]</a>    <span class="k">def</span> <span class="nf">_add_forwardHook_to_layers</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">hook_function</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Internal function used to add a passed hook_function to layers. </span>
<span class="sd">        </span>
<span class="sd">        Users should not use this function, it only exists due to recursion/class structure nuances.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># Iterate through all the attached modules register the hook function with that layer</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">network</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span> <span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_add_forwardHook_to_layers</span><span class="p">(</span> <span class="n">layer</span><span class="p">,</span> <span class="n">hook_function</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">handle</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span> <span class="n">hook_function</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">registered_forward_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">handle</span><span class="p">)</span></div>


<div class="viewcode-block" id="Network._add_backwardHook_to_layers"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network._add_backwardHook_to_layers">[docs]</a>    <span class="k">def</span> <span class="nf">_add_backwardHook_to_layers</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">hook_function</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Internal function used to add a passed hook_function to layers. </span>
<span class="sd">        </span>
<span class="sd">        Users should not use this function, it only exists due to recursion/class structure nuances.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># Iterate through all the attached modules register the hook function with that layer</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">network</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span> <span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_add_backwardHook_to_layers</span><span class="p">(</span> <span class="n">layer</span><span class="p">,</span> <span class="n">hook_function</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">handle</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">register_full_backward_hook</span><span class="p">(</span> <span class="n">hook_function</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">registered_backward_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">handle</span><span class="p">)</span></div>


<div class="viewcode-block" id="Network.add_hook_activation"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.add_hook_activation">[docs]</a>    <span class="k">def</span> <span class="nf">add_hook_activation</span><span class="p">(</span> <span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Single comprehensive function that adds hooks/attributes necessary to track activation values of the network during training/testing.</span>

<span class="sd">        Args:</span>
<span class="sd">            No Arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No return value. Hooks will be registered and attributes initialized.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">track_activation_setup</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations_keys_ordered</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span> <span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39; Define the function that the hook will execute with the standard hook input variables</span>
<span class="sd">                To track activatoins, we create a dictionary with the layer objects as keys, and the output activation values as values</span>
<span class="sd">                Also assemble an ordered list of the activation layer keys for organized iteration later</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="c1"># key = id(module)</span>
                <span class="n">key</span> <span class="o">=</span> <span class="n">module</span>
                <span class="c1"># Note: Based on https://stackoverflow.com/questions/55266154/pytorch-preferred-way-to-copy-a-tensor , </span>
                <span class="c1"># torch.empty_like(output).copy_(output).cpu() is fastest way to copy tensor to cpu</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_activations</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations_keys_ordered</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations_keys_ordered</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">WARNING: HOOK ERROR</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
                    <span class="k">raise</span> <span class="n">e</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_add_forwardHook_to_layers</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">hook_fn</span><span class="p">)</span></div>
    

<div class="viewcode-block" id="Network.add_hook_activationGrad"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.add_hook_activationGrad">[docs]</a>    <span class="k">def</span> <span class="nf">add_hook_activationGrad</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">layerTarget</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Single comprehensive function that adds hooks/attributes necessary to track activation values of the network during training/testing.</span>

<span class="sd">        Args:</span>
<span class="sd">            layerTarget (nn.Module layer, optional): Single layer to track activation gradients for. If not specified, all layers are tracked. Testing of this specific optinal input was not comprehensive, may need tweaking to assure proper cleanup and workflow with other functions.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No return value. Hooks will be registered and attributes initialized.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">track_activationGrad_setup</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_raw</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_keys_ordered</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">def</span> <span class="nf">hook_fn_backward</span><span class="p">(</span> <span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39; Define the function that the hook will execute with the standard hook input variables</span>
<span class="sd">                To track activations, we create a dictionary with the layer objects as keys, and the output activation values as values</span>
<span class="sd">                Also assemble an ordered list of the activation layer keys for organized iteration later</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="c1"># key = id(module)</span>
                <span class="n">key</span> <span class="o">=</span> <span class="n">module</span>
                <span class="c1"># Note: Based on https://stackoverflow.com/questions/55266154/pytorch-preferred-way-to-copy-a-tensor , </span>
                <span class="c1"># torch.empty_like(output).copy_(output).cpu() is fastest way to copy tensor to cpu</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">cpu</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_keys_ordered</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_keys_ordered</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">WARNING: HOOK ERROR</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
                    <span class="k">raise</span> <span class="n">e</span>
        
        <span class="c1"># Define a forward hook which will add the backwards hook onto the activation tensor, once it is created</span>
        <span class="c1"># This 2-hook setup is necessary because we have to wait until output is generated to add a hook</span>
        <span class="k">def</span> <span class="nf">hook_fn_forward_tensor</span><span class="p">(</span> <span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39; Define a forward hook which will add the backwards hook onto the activation tensor, once it is created</span>
<span class="sd">                This 2-hook setup is necessary because we have to wait until output is generated to add a hook</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">handle_t</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">hook_fn_backward_tensor</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">registered_backward_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle_t</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">hook_fn_backward_tensor</span><span class="p">(</span> <span class="n">grad</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39; Define a backward hook which will keep track of the activation gradients</span>
<span class="sd">                This 2-hook setup is necessary because we have to wait until output is generated to add a hook</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="c1"># Note: Based on https://stackoverflow.com/questions/55266154/pytorch-preferred-way-to-copy-a-tensor , </span>
                <span class="c1"># torch.empty_like(output).copy_(output).cpu() is fastest way to copy tensor to cpu</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_raw</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_raw</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_raw</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">WARNING: HOOK ERROR</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
                    <span class="k">raise</span> <span class="n">e</span>
        

        <span class="k">if</span> <span class="n">layerTarget</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">layerTarget</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span> <span class="n">hook_fn_forward_tensor</span><span class="p">)</span>
            <span class="c1"># handle = layerTarget.register_backward_hook(hook_fn_backward)</span>
            <span class="c1"># self.registered_backward_hooks.append(handle)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_add_backwardHook_to_layers</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">hook_fn_backward</span><span class="p">)</span></div>


<div class="viewcode-block" id="Network.track_activations_cleanup"><a class="viewcode-back" href="../../../toast.heat.network.html#toast.heat.network.Network.track_activations_cleanup">[docs]</a>    <span class="k">def</span> <span class="nf">track_activations_cleanup</span><span class="p">(</span> <span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;Helper function to easily cleanup all activation-tracking varaibles/hooks after training.</span>

<span class="sd">        May be necessary to move network to/from GPU or to save network, as the activation tracking implemention is unpickleable.</span>

<span class="sd">        Args:</span>
<span class="sd">            No Arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            No return value. Hooks will be unregistered and attributes cleared.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">track_activation_setup</span>     <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">track_activationGrad_setup</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remove_all_hooks</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activations_keys_ordered</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_raw</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_activationsGrad_keys_ordered</span> <span class="o">=</span> <span class="p">[]</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Cybernetic Implantable Devices.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>